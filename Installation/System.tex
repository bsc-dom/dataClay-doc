\chapterimage{Client.jpg} % Chapter heading image

\chapter{Deployment}\index{installation}
\label{sec:SystemInstall}

This chapter explains how to perform a dataClay installation. First, we briefly describe the dataClay architecture for the better understanding of its different components and their interactions. Thereafter, we show how to deploy a minimum dataClay installation based on Dockers\index{dockers}, and how to extrapolate this basic setup to more complex scenarios.

In case you have some other needs not addressed in this chapter, please contact us by email:

\texttt{\href{mailto:support-dataclay@bsc.es}{support-dataclay@bsc.es}}



\section{dataClay architecture}
The architecture of dataClay is composed by two main components: the Logic Module\index{logic module} and the Data Service\index{data service}. The Logic Module is a central repository that handles object metadata and management information. The Data Service is a distributed object store that handles object persistence and execution requests.

\begin{figure}[h]
\centering\includegraphics[scale=0.4]{Installation/dataClayOverview}
\caption{dataClay overview}
\label{fig:dataClayOverview}
\end{figure}

\subsection{Logic Module}\index{logic module}
The Logic Module is a unique centralized component that keeps track of every object metadata such as: its unique identifier, (replica) locations, and the dataset it is associated with. 

In addition, the Logic Module is in charge of management info, comprising: accounting, namespaces and datasets, permissions (contracts) and registered class models. That is, the information that can be registered in the system using the dataClay command line utility as shown in section \ref{sec:dClayTool}.

Furthermore, the Logic Module is the entry point for any user application, which must authenticate in the system to create a working session and gain permission to interact with the components of the Data Service.

\subsection{Data Service}\index{data service}\index{backend}
\label{sec:DataService}
The Data Service is deployed as a set of multiple backends. Any of these backends handles a subset of objects as well as the execution requests aiming to them. This means that every backend has an \textit{Execution Environment} for all supported languages (currently Java and Python) and an associated \textit{Storage System} to handle object persistence. In the case of Python, where multi-threading cannot be managed as Java does, it is possible to deploy multiple \textit{Execution Environments} sharing a single \textit{Storage System} thus enabling applications to exploit parallelism.

In order to enable \textit{Execution Environments} to handle any kind of object and execution request, the Logic Module is in charge to deploy registered classes to every Data Service backend. In this way, every backend can load stored objects as class instances and execute class methods on them corresponding to upcoming execution requests.

This means that when an application initializes a session with dataClay, it first establishes a connection with the Logic Module and obtains information about the available Data Service backends. At this point, the application is enabled to interact with the Data Service backends through stub classes (retrieved with the dataClay command line utility, section \ref{sec:dClayTool}) by submitting execution requests directly to them.



\section{Deployment with containers \index{docker}}

Keeping in mind the dataClay architecture, hereafter we show how to deploy a dataClay installation based on Docker containers.

A container image is a lightweight, stand-alone, executable package of a piece of software that includes everything needed to run it: code, runtime, system tools, system libraries, settings. 

In this way, we populate different Docker images corresponding to the main components of the dataClay architecture: the Logic Module and the Data Service. The latter actually comprises one image for each supported language since the corresponding execution requests are handled by separated containers (one per language). 

In order to retrieve these images and orchestrate dataClay services properly, following sections show different scenarios based on the standard \textit{docker-compose}\index{docker-compose} tools.

\subsection{Single node installation}
\label{sec:SingleNodeInstall}
This first dataClay installation assumes that all services will run locally on a single node (e.g. your own laptop).

To this end, we will guide you through the installation process by looking at the details of the following \textit{docker-compose} YAML definition:

\begin{tBox}
 \begin{lstlisting}[language=docker-compose-2, frame=none]
version: '3.4'
services:
  dcinitializer:
    image: "bscdataclay/initializer"
    command: "--wait-for-python-ds 1 --wait-for-java-ds 1"
    depends_on:
      - logicmodule
      - dsjava
      - dspython
    environment:
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
      - USER=${USER:-defaultUser}
      - PASS=${PASS:-defaultPass}
      - DATASET=${DATASET:-defaultDS}
      - GIT_PYTHON_MODELS_URLS=https://gitlab.bsc.es/model.git
      - PYTHON_NAMESPACES=Namespace
    healthcheck:
      interval: 5s
      retries: 10
      test: [ "CMD-SHELL", "/dataclay-initializer/health_check.sh" ]

  logicmodule:
    image: "bscdataclay/logicmodule"
    ports:
      - "11034:11034"
    environment:
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
      - DATACLAY_ADMIN_USER=admin
      - DATACLAY_ADMIN_PASSWORD=admin
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/javaclay/health_check.sh"]
         
  dsjava:
    image: "bscdataclay/dsjava"
    ports:
      - "2127:2127"
    depends_on:
      - logicmodule
    environment:
      - DATASERVICE_NAME=DS1
      - DATASERVICE_JAVA_PORT_TCP=2127
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/javaclay/health_check.sh"]
       
  dspython:
    image: "bscdataclay/dspython"
    ports:
      - "6867:6867"
    depends_on:
      - logicmodule
      - dsjava
    environment:
      - DATASERVICE_NAME=DS1
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
      - DATASERVICE_PYTHON_PORT_TCP=6867
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/pyclay/health_check.sh"]
 \end{lstlisting}
\end{tBox}

Before starting, the first step is to download the required images executing the following command from the directory where this \textit{docker-compose} file resides:
\begin{tBox}
 \begin{bash}
  > docker-compose pull
 \end{bash}
\end{tBox}

At this point, following subsections detail the different parts of the file and which ones can be customized.

\subsubsection{Initializer}
The \textit{initializer} service facilitates the initialization of a dataClay instance by enabling the user to indicate the classes to be registered, which may reside locally or in a remote repository. For this container, the host and port of the Logic Module must be indicated in the corresponding environment variables \texttt{LOGICMODULE\_HOST} and \texttt{LOGICMODULE\_PORT\_TCP} (see next subsection), as well as all the information required to register the classes. In particular, the user name and password must be given in variables \texttt{USER} and \texttt{PASS}. The dataset where objects will be created corresponds to variable \texttt{DATASET}, and the namespace where the model will be registered shall be indicated in \texttt{JAVA\_NAMESPACES} or \texttt{PYTHON\_NAMESPACES}. Finally, the classes to be registered can be indicated in the following environment variables: \texttt{JAVA\_MODELS\_PATH} or \texttt{PYTHON\_MODELS\_PATH}, if they are stored locally, or in the variables \texttt{GIT\_JAVA\_MODELS\_URLS}/\texttt{GIT\_PYTHON\_MODELS\_URLS}, and \texttt{GIT\_JAVA\_MODELS\_PATHS}/\texttt{GIT\_PYTHON\_MODELS\_PATHS} if they are available in a GIT repository. In all cases several values can be indicated, separated by spaces.

Alternatively, models can also be imported from other dataClay instances which already have them registered. For this, the following environment variables shall be used: 
\begin{itemize}
   \item[] \texttt{IMPORT\_MODELS\_FROM\_EXTERNAL\_DC\_HOSTS} 
   \item[] \texttt{IMPORT\_MODELS\_FROM\_EXTERNAL\_DC\_PORTS}
   \item[] \texttt{IMPORT\_MODELS\_FROM\_EXTERNAL\_DC\_NAMESPACES}
\end{itemize}

\subsubsection{Logic Module}
The \textit{logicmodule} service corresponds to the Logic Module. It is possible to customize the default Logic Module port, which is currently set to 11034 through environment variable \texttt{LOGICMODULE\_PORT\_TCP} and is mapped to host in \texttt{ports: - ``11034:11034''}. 

In this way, the Logic Module will publish its service at: \texttt{localhost:11034}. In section \ref{sec:ClientConfigFiles} it is described how to define the proper configuration files for user's applications considering this info.

\subsubsection{Data Service Backend - Java}
The \textit{dsjava}  service corresponds to the Java container of the Data Service Backend. Every Data Service backend is tagged with a name so containers for all supported languages can be defined as part of the same backend. This is specially useful to, for instance, define a unique database shared by different execution environments. In this case, the Java container is configured to be part of the Data Service backend called \textit{DS1} as defined via the \texttt{DATASERVICE\_NAME} variable. Furthermore, it is also necessary to specify the port that will be used to handle Java execution requests: \texttt{DATASERVICE\_JAVA\_PORT\_TCP=2127}.

Finally, we also need to specify the address of the Logic Module to enable this Java container to populate its service. To this end, we use the same variables and values as in the Logic Module service (\textit{logicmodule}): \texttt{LOGICMODULE\_HOST=logicmodule} and \texttt{LOGICMODULE\_PORT\_TCP=11034}.

\subsubsection{Data Service Backend - Python}
For Python, we only need to attach the container to a Data Service backend with Java support. In this case, we attach the Python container to \textit{DS1} Data Service backend through the \texttt{DATASERVICE\_NAME} variable. Furthermore, it is also necessary to specify the port that will be used to handle Java execution requests: \texttt{DATASERVICE\_PYTHON\_PORT\_TCP=6867}.

Finally, we also need to specify the address of the Logic Module to enable this Python container to populate its service. To this end, we use the same variables and values as in the Logic Module service (\textit{logicmodule}): \texttt{LOGICMODULE\_HOST=logicmodule} and \texttt{LOGICMODULE\_PORT\_TCP=11034}.

\subsection{Cluster installation}

If you want to deploy dataClay on a cluster of N nodes, you can create different \textit{docker-compose} files for each node depending on the setup you want.

In this section we describe a setup for a 3 node cluster with 1 node for the Logic Module and 2 nodes for Data Service backends. You can easy extrapolate this scenario to more complex ones, but always keeping in mind the following considerations/constraints for the current version of dataClay:

\begin{enumerate}
 \item The Logic Module is unique in the system. This means that only one of the nodes should have a docker-compose file with the Logic Module section.
 \item In this case, all services must be exposed using ``host'' network mode in order to make them visible and discoverable between different nodes and from the client application.
\end{enumerate}

\subsubsection{Node 1 - Logic Module}

The docker-compose file for the first node defining the Logic Module. Notice that using host network we do not need to map its port.

\begin{tBox}
 \begin{lstlisting}[language=docker-compose-2, frame=none]
version: '3.4'
services:
  logicmodule:
    image: "bscdataclay/logicmodule"
    ports:
      - "11034:11034"
    environment:
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
      - DATACLAY_ADMIN_USER=admin
      - DATACLAY_ADMIN_PASSWORD=admin
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/javaclay/health_check.sh"]
 \end{lstlisting}
\end{tBox}

\subsubsection{Node 2 - Backend 1}

This node runs the Data Service backend \textit{DS1}, as specified through the \texttt{DATASERVICE\_NAME} variable.

Notice that the only variable that needs to be manually defined is \texttt{LOGICMODULE\_HOST}, which will be the host name of Node 1 where the Logic Module is deployed.

\begin{tBox}
 \begin{lstlisting}[language=docker-compose-2, frame=none]
version: '3.4'
services:
 dsjava:
    image: "bscdataclay/dsjava"
    ports:
      - "2127:2127"
    depends_on:
      - logicmodule
    environment:
      - DATASERVICE_NAME=DS1
      - DATASERVICE_JAVA_PORT_TCP=2127
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/javaclay/health_check.sh"]
       
  dspython:
    image: "bscdataclay/dspython"
    ports:
      - "6867:6867"
    depends_on:
      - logicmodule
      - dsjava
    environment:
      - DATASERVICE_NAME=DS1
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
      - DATASERVICE_PYTHON_PORT_TCP=6867
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/pyclay/health_check.sh"]
 \end{lstlisting}
\end{tBox}

\subsubsection{Node 3 - Backend 2}

Analogously to Node 2, this node runs the Data Service backend \textit{DS2}, as specified through the \texttt{DATASERVICE\_NAME} variable.

Finally, the only variable that needs to be manually defined is \texttt{LOGICMODULE\_HOST} with the host name of the Node 1 where Logic Module is deployed.

\begin{tBox}
 \begin{lstlisting}[language=docker-compose-2, frame=none]
version: '3.4'
services:
 dsjava2:
    image: "bscdataclay/dsjava"
    ports:
      - "2128:2128"
    depends_on:
      - logicmodule
    environment:
      - DATASERVICE_NAME=DS2
      - DATASERVICE_JAVA_PORT_TCP=2128
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/javaclay/health_check.sh"]
       
  dspython2:
    image: "bscdataclay/dspython"
    ports:
      - "6868:6868"
    depends_on:
      - logicmodule
      - dsjava
    environment:
      - DATASERVICE_NAME=DS2
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
      - DATASERVICE_PYTHON_PORT_TCP=6868
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/pyclay/health_check.sh"]
 \end{lstlisting}
\end{tBox}


\subsection{Enabling Python parallelism}
\label{sec:PythonParallelism}

In Section~\ref{sec:PythonConsiderationsExecutionEnvironment} we explain that the implementation details of the CPython Global Interpreter Lock forces that only one thread can execute Python code at once. However, and as introduced in Section~\ref{sec:DataService}, we can mitigate this problem by configuring dataClay to deploy multiple Python execution environments (backends) on a single node. The example below shows two Python execution environments that will load/store objects from the same Data Service \textit{DS1} ((\textit{dspython1, dspython2}).

\begin{tBox}
 \begin{lstlisting}[language=docker-compose-2, frame=none]
version: '3.4'
services:
  logicmodule:
    image: "bscdataclay/logicmodule"
    ports:
      - "11034:11034"
    environment:
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
      - DATACLAY_ADMIN_USER=admin
      - DATACLAY_ADMIN_PASSWORD=admin
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/javaclay/health_check.sh"]
         
  dsjava:
    image: "bscdataclay/dsjava"
    ports:
      - "2127:2127"
    depends_on:
      - logicmodule
    environment:
      - DATASERVICE_NAME=DS1
      - DATASERVICE_JAVA_PORT_TCP=2127
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/javaclay/health_check.sh"]
       
  dspython:
    image: "bscdataclay/dspython"
    ports:
      - "6867:6867"
    depends_on:
      - logicmodule
      - dsjava
    environment:
      - DATASERVICE_NAME=DS1
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
      - DATASERVICE_PYTHON_PORT_TCP=6867
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/pyclay/health_check.sh"]
       
  dspython2:
    image: "bscdataclay/dspython"
    ports:
      - "6868:6868"
    depends_on:
      - logicmodule
      - dsjava
    environment:
      - DATASERVICE_NAME=DS1
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
      - DATASERVICE_PYTHON_PORT_TCP=6868
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/pyclay/health_check.sh"]
       
 \end{lstlisting}
\end{tBox}

\subsection{Tuning dataClay}

dataClay allows to tune some specific settings (detailed in next sections) using either environment variables or a property file located on a specific path. 

In the application/client side the default path of this file is \texttt{./cfgfiles/global.properties} and can be also defined via the environment variable \texttt{DATACLAY\_GLOBAL\_CONFIG}.

In the server-side the default path is the same, but following previous examples with Docker containers we must define a \textit{volume} to load it. Given the first docker-compose file for a local installation, and assuming that there is a property file located at \texttt{./cfgfiles/global.properties} (relative to the docker-compose file), the volume can be mounted in a per-service basis as illustrated below:

\begin{tBox}
 \begin{lstlisting}[language=docker-compose-2, frame=none]
version: '3.4'
services:
  logicmodule:
    image: "bscdataclay/logicmodule"
    ports:
      - "11034:11034"
    environment:
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
      - DATACLAY_ADMIN_USER=admin
      - DATACLAY_ADMIN_PASSWORD=admin
    volumes:
      - ./prop/global.properties:/usr/src/dataclay/javaclay/cfgfiles/global.properties:ro
      - ./prop/log4j2.xml:/usr/src/dataclay/javaclay/log4j2.xml:ro
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/javaclay/health_check.sh"]
         
  dsjava:
    image: "bscdataclay/dsjava"
    ports:
      - "2127:2127"
    depends_on:
      - logicmodule
    environment:
      - DATASERVICE_NAME=DS1
      - DATASERVICE_JAVA_PORT_TCP=2127
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
    volumes:
      - ./prop/global.properties:/usr/src/dataclay/javaclay/cfgfiles/global.properties:ro
      - ./prop/log4j2.xml:/usr/src/dataclay/javaclay/log4j2.xml:ro
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/javaclay/health_check.sh"]
       
  dspython:
    image: "bscdataclay/dspython"
    ports:
      - "6867:6867"
    depends_on:
      - logicmodule
      - dsjava
    environment:
      - DATASERVICE_NAME=DS1
      - LOGICMODULE_PORT_TCP=11034
      - LOGICMODULE_HOST=logicmodule
      - DATASERVICE_PYTHON_PORT_TCP=6867
    volumes:
      - ./prop/global.properties:/usr/src/dataclay/pyclay/cfgfiles/global.properties:ro
    healthcheck:
       interval: 5s
       retries: 10
       test: ["CMD-SHELL", "/usr/src/dataclay/pyclay/health_check.sh"]
 \end{lstlisting}
\end{tBox}

\subsection{Singularity}
dataClay can also be deployed using Singularity, by using the \texttt{singularity pull} command (for more information see the \href{https://sylabs.io/guides/3.0/user-guide/build_a_container.html#downloading-an-existing-container-from-docker-hub}{Singularity official guide}): 

\texttt{singularity pull docker://bscdataclay/logicmodule}\\
\texttt{singularity pull docker://bscdataclay/dsjava}\\
\texttt{singularity pull docker://bscdataclay/dspython}

Each container can be orchestrated by using \href{https://singularityhub.github.io/singularity-compose}{Singularity Compose} (e.g. by manually porting the configurations contained in the \texttt{docker-compose.yml} example files).

\subsection{Memory Management and Garbage Collection}\index{memory management}\index{garbage collection}

In section \ref{sec:GarbageCollection} we have introduced that dataClay runs some processes to keep memory and disk usage in a healthy state. On the one hand, flushing objects from memory to disk when memory usage reaches a certain threshold. On the other hand, keeping track of reference counters to detect objects that are no longer accessible so they can be removed from the system.

To control the impact of these processes on the system performance, dataClay provides the administrator with the capability to configure the following parameters via environment variables or the \textit{global.properties} file. Notice that time parameters are always expressed in milliseconds, except stated otherwise.

\begin{table}[H]
\footnotesize
\begin{tBox}
\centering
\begin{tabular}{p{67mm} | p{27mm} |  >{\raggedright\arraybackslash}p{40mm}}
\textbf{property} & \textbf{default value} & \textbf{description} \\
\hline
\verb MEMMGMT_CHECK_TIME_INTERVAL & 5000 (5 seconds) & Periodicity to check memory usage. \\
\hline
\verb MEMMGMT_EASE_FRACTION & 0 & Minimum fraction for an object to be in memory. \\
\hline 
\verb MEMMGMT_MIN_OBJECT_TIME & 200 & Minimum time for an object to be in memory. \\
\hline
\verb MEMMGMT_PRESSURE_FRACTION & 0.7 (70\%) & Fraction of memory usage from which to consider that it is under pressure. \\
\hline
\verb GLOBALGC_CHECK_REMOTE_PENDING & 2000 & Interval to check pending references to notify to other nodes.\\
\hline
\verb GLOBALGC_CHECK_TIME_INTERVAL & 86400000 (1 day) & Periodicity to check and collect objects from underlying storage. \\
\hline
\verb GLOBALGC_COLLECT_TIME_INTERVAL & 10000 & Interval to check objects to clean. \\
\hline
\verb GLOBALGC_COLLECTOR_INITIAL_DELAY_HOURS & 0 & Garbage collector initial delay in hours. \\
\hline 
\verb GLOBALGC_MAX_OBJECTS_TO_COLLECT_ITERATION & 1000 & Maximum number of objects to clean per iteration. \\
\hline
\verb GLOBALGC_PROCESS_COUNTINGS_INTERVAL & 1000 & Interval to check pending references to count. \\
\end{tabular}
\label{table:GarbageCollection}
\end{tBox}
\end{table}

%An example of the \textit{global.properties} file with non-default values would be as follows:
%
%\begin{tBox}
% \begin{bash}
%MEMMGMT_PRESSURE_FRACTION=0.8
%MEMMGMT_CHECK_TIME_INTERVAL=8000
%GLOBALGC_CHECK_TIME_INTERVAL=1000000
 %\end{bash}
%\end{tBox}
%
\PREFETCH{
\subsubsubsection{Data Prefetching}\index{prefetching}
\label{sec:prefetchingTuning}
As explained in Section \ref{sec:prefetching}, the user can activate data prefetching in dataClay in order to improve the application performance. In order to do so, the following property should be set to true in the \textit{global.properties} file of both the logic module and data services when dataClay is installed.

\begin{table}[H]
\footnotesize
\begin{tBox}
\centering
\begin{tabular}{p{57mm} | p{27mm} |  >{\raggedright\arraybackslash}p{50mm}}
\textbf{property} & \textbf{default value} & \textbf{description} \\
\hline
\verb PREFETCHING_ENABLED & FALSE & Activate or deactivate prefetching in dataClay. \\
\end{tabular}
\label{table:GarbageCollection}
\end{tBox}
\end{table}

Keep in mind that prefetching should also be activated when registering an application using dataClay tool, as explained in Section \ref{sec:newModel}.
}